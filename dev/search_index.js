var documenterSearchIndex = {"docs":
[{"location":"1_algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"","category":"section"},{"location":"1_algorithms/#Volatility","page":"Algorithms","title":"Volatility","text":"","category":"section"},{"location":"1_algorithms/","page":"Algorithms","title":"Algorithms","text":"There are two builtin algorithms that purely estimate volatility. These are:","category":"page"},{"location":"1_algorithms/","page":"Algorithms","title":"Algorithms","text":"simple_volatility this estimates the volatility for each stock given a grid of sampling times. If a grid of sampling times is not input then one is estimated optimally (using a formula suggested by Zhang, Mykland & Aït-Sahalia 2005).\ntwo_scales_volatility This estimates volatility over two different timescales. One short duration (so alot of the measured variation will be from microstructure noise) and one longer duration (so little of the measured variation is from microstructure noise). Then it combines the two estimates to get an estimate of volatility and also of microstructure noise.","category":"page"},{"location":"1_algorithms/","page":"Algorithms","title":"Algorithms","text":"In addition it is generally possible to infer volatility from the covariance matrix estimates. Given a covariance matrix over some interval it is possible to extract the variance of each stock's price and then determine the volatility from that. In most cases when one of the following covariance estimation function is use it is this measure of volatility that is put into the CovarianceMatrix struct.","category":"page"},{"location":"1_algorithms/#Covariance","page":"Algorithms","title":"Covariance","text":"","category":"section"},{"location":"1_algorithms/","page":"Algorithms","title":"Algorithms","text":"There are five algorithms for estimating covariances.","category":"page"},{"location":"1_algorithms/","page":"Algorithms","title":"Algorithms","text":"The simple_covariance function estimates a covariance matrix in the basic way using a given timegrid. Users can input their own timegrid, their own spacing that will be used to gather observations or choose to use refresh times (which will likely be biased and so is not recommended). If the user does not do this then by default the spacing will be the average (across assets) optimal spacing that is calculated in the simple_volatility function.\nThe preaveraged_covariance function. This first preaverages price updates over certain intervals and then estimates the correlations using the averaged series. As a result of averaging the microstructure noise is reduced and the correlations are more accurate as a result. As the volatilities of preaveraged returns are artificially low we use two scales volatility estimates for volatility in this case (Christensen, Podolskij and Vetter 2013).\nThe two_scales_covariance function. This calculates volatilities using the two_scales_volatility estimator. It then calculated pairwise correlations by comparing the two scales volatility of different linear combinations of the two assets (Ait-Sahalia, Fan and Xiu 2010).\nThe spectral_covariance function - The spectral local method of moments technique  (Bibinger, Hautsch, Malec, and Reiss 2014) starts by breaking the trading period into equally sized subintervals. Given each subinterval we  compute a spectral statistic matrix by using a weighted sum of the returns within that interval. We calculate these weights by means of an orthogonal sine function with some spectral frequency j. Then we gather many different spectral statistic matrices by doing this repeatedly with different spectral frequencies. Our estimate of the covariance matrix is then calculated as the average of these spectral statistics.\nThe bnhls_covariance function - The multivariate realised kernel (Barndorff-Nielsen, Hansen, Lunde, and Shephard 2008 - sometimes called the BNHLS method after the authors) is an algorithm that is designed to provide consistent PSD covariance estimates despite settings where there is microstructure noise (that may not be independent of the underlying price process) and asyncronously traded assets. It is a refinement of an earlier algorithm, the univariate realised kernel estimator, which is faster converging but relies an assumption of independence between microstructure noise and the underlying price process.","category":"page"},{"location":"1_algorithms/#Regularisation","page":"Algorithms","title":"Regularisation","text":"","category":"section"},{"location":"1_algorithms/","page":"Algorithms","title":"Algorithms","text":"There are four inbuilt regularisation algorithms, identity_regularisation, eigenvalue_clean, nearest_psd_matrix and nearest_correlation_matrix. The first three of these can be applied to either the covariance matrix or to the correlation matrix while the fourth can only be applied to the correlation matrix. If input as a regularsation_method to a covariance estimation function, these methods can regularise a covariance matrix before it is split into a correlation matrix and volatilities. It can also be applied purely to the resultant correlation matrix. These regularisation techniques can also be applied directly to a CovarianceMatrix struct either on the correlation matrix or covariance matrix (in which case a covariance matrix is constructed, regularised and then split into a correlation matrix and volatilities that are then placed in a CovarianceMatrix struct).","category":"page"},{"location":"1_algorithms/","page":"Algorithms","title":"Algorithms","text":"The regularisation techniques are:","category":"page"},{"location":"1_algorithms/","page":"Algorithms","title":"Algorithms","text":"identity_regularisation -  This regularises a covariance (or correlation) by averaging it with an identity matrix of the same dimensions (Ledoit and Wolf 2001).\neigenvalue_clean - This splits a covariance (or correlation) matrix into its eigenvalue decomposition. Then the distribution of eigenvalues that would be expected if it were a random matrix is computed. Any eigenvalues that are sufficiently small that they could have resulted are averaged together which shrinks their impact while the covariance matrix is still psd (Laloux, Cizeau, Bouchaud and Potters 1999).\nnearest_psd_matrix - This maps an estimated matrix to the nearest psd matrix (Higham 2002).\nnearest_correlation_matrix - This maps an estimated correlation matrix to the nearest psd matrix. And then the nearest unit diagonal (with offdiagonals less than one in absolute value) matrix. Then the nearest psd matrix and so on until it converges. The result is the nearest valid correlation matrix.","category":"page"},{"location":"2_WritingCode/#Using-HighFrequencyCovariance","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"","category":"section"},{"location":"2_WritingCode/#Main-Structs","page":"Using HighFrequencyCovariance","title":"Main Structs","text":"","category":"section"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"HighFrequencyCovariance has two main structs. The first is CovarianceMatrix which is:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"mutable struct CovarianceMatrix{R<:Real}\n    correlation::Hermitian{R}\n    volatility::Array{R,1}\n    labels::Array{Symbol,1}\nend","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"A CovarianceMatrix struct thus contains three elements. A correlation matrix, a volatility vector and a vector that labels each row/column of the correlation matrix and each row of the volatility vector. Note that an actual covariance matrix is not stored but can be calculated over some interval with the function:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"covariance(cm::CovarianceMatrix, duration::Real)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"The second main struct is a SortedDataFrame which is:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"struct SortedDataFrame\n    df::DataFrame\n    time::Symbol\n    grouping::Symbol\n    value::Symbol\n    groupingrows::Dict{Symbol,Array{Int64,1}}\nend","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"This presorts a dataframe by time and adds in an index (groupingrows) for each asset. Together these measures allow the covariance estimation functions to run faster as they can efficiently access the data. The other struct elements are the labels of the columns of interest in the dataframe.","category":"page"},{"location":"2_WritingCode/#Loading-in-data","page":"Using HighFrequencyCovariance","title":"Loading in data","text":"","category":"section"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"We first load our data into a dataframe. As an example we have a dataframe of price updates (like that in df below). Then we can put our data into a SortedDataFrame by putting the dataframe and the names of the time, label and value columns into the constructor:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"using HighFrequencyCovariance\nusing DataFrames\ndf = DataFrame(:stock => [:A,:B,:A,:A,:A,:B,:A,:B,:B], :time => [1,2,3,4,5,5,6,7,8],\n               :logprice => [1.01,2.0,1.011,1.02,1.011,2.2,1.0001,2.2,2.3])\nts = SortedDataFrame(df, :time, :stock, :logprice)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"In a real setting this is how we would turn our dataframe of data into a SortedDataFrame.","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"For the suceeding sections it is useful to get more realistic time series data. So we will generate some Monte Carlo data here using the generate_random_path function which generates a random correlation matrix, volatilities, price update times and microstructure noises and generates a SortedDataFrame from a random time series consistent with these.","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"using HighFrequencyCovariance\ndims = 4\nticks = 4000\nts, true_covar, true_micro_noise, true_update_rates = generate_random_path(dims, ticks)","category":"page"},{"location":"2_WritingCode/#Estimating-a-covariance-matrix","page":"Using HighFrequencyCovariance","title":"Estimating a covariance matrix","text":"","category":"section"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"As this is a Monte Carlo we already have the true CovarianceMatrix in the true_covar variable. As we don't have this in applied settings we will disregard this for now and try to estimate it using our data from ts_data:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"assets              = get_assets(ts_data)\nsimple_estimate     = simple_covariance(ts_data, assets)\nbnhls_estimate      = bnhls_covariance(ts_data, assets)\nspectral_estimate   = spectral_covariance(ts_data, assets)\npreav_estimate      = preaveraged_covariance(ts_data, assets)\ntwo_scales_estimate = two_scales_covariance(ts_data, assets)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"Now we may be particularly interested in one of the estimates, for instance the bnhls estimate. We can first see if the correlation matrix is valid:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"valid_correlation_matrix(bnhls_estimate)\n# true","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"and fortunately it is. We could also examine the others similarly and see that they all deliver valid correlation matrices. One thing we might try then is to average over all of the more advanced methods and use the result as our correlation matrix estimate. This is easy to achieve by using the combine_covariance_matrices function.","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"matrices = [spectral_estimate, preav_estimate, two_scales_estimate, bnhls_estimate]\ncombined_estimate = combine_covariance_matrices(matrices)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"Now we can compare how close each of the estimates is to the true correlation matrix. We can do this by examining the mean absolute difference between estimated correlations.","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"calculate_mean_abs_distance(true_covar, combined_estimate)\n# (Correlation_error = 0.16456385637458595, Volatility_error = 0.004544707431296664)\ncalculate_mean_abs_distance(true_covar, simple_estimate)\n# (Correlation_error = 0.448889645637147, Volatility_error = 0.011006475712027963)\ncalculate_mean_abs_distance(true_covar, bnhls_estimate)\n# (Correlation_error = 0.23409341003189574, Volatility_error = 0.001093802689437351)\ncalculate_mean_abs_distance(true_covar, spectral_estimate)\n# (Correlation_error = 0.22234619862055216, Volatility_error = 0.00385672089771947)\ncalculate_mean_abs_distance(true_covar, preav_estimate)\n# (Correlation_error = 0.14664816908395706, Volatility_error = 0.0017134509530432648)\ncalculate_mean_abs_distance(true_covar, two_scales_estimate)\n# (Correlation_error = 0.3577017001321916, Volatility_error = 0.0017134509530432648)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"We can see that in this particular case the correlation matrix calculated with preaveraging performed the best.","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"Now examining the data we can see that we have one asset that trades much more frequently than the others.","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"ticks_per_asset(ts_data)\n# Dict{Symbol,Int64} with 4 entries:\n#  :asset_4 => 6848\n#  :asset_3 => 6588\n#  :asset_2 => 2630\n#  :asset_1 => 3934","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"While we have 6848 price updates for asset_4 we only have 2630 for asset_2. Potentially we could improve the bnhls estimate if we use a blocking and regularisation technique (Hautsch, Kyj and Oomen  2012).","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"We can start this by first making a dataframe detailing what assets should be in what block. We will generate a new block if the minimum number of ticks of a new block has 20% more ticks than the minimum of the previous:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"new_block_threshold = 1.2\nblocking_frame = put_assets_into_blocks_by_trading_frequency(\n                        ts_data, new_block_threshold, bnhls_covariance)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"This blocking_frame is a regular dataframe with six columns where each row represents a different estimation. The order of the rows is the order of estimations (so the results of later estimations may overwrite earlier ones). The first column is named :assets and has the type Set{Symbol} which represents the assets in each estimation. The second column contains the function that will be used in the estimation of that block. The third column has the name :optional_parameters and is of type NamedTuple that can provide optional parameters to the covariance function in the second column. Every covariance estimation has a function signature with two common arguments before the semicolon (For a SortedDataFrame and a vector of symbols representing what assets to use). There can also be a number of named optional arguments which can be sourced from a NamedTuple. The blockwise_estimation function then estimates a block with the line","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"i = 1\nblocking_frame[i,:f](ts_data, collect(blocking_frame[i,:assets]);\n                     blocking_frame[i,:optional_parameters]... )","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"Thus a user can insert a named tuple containing whatever optional parameters are used by the function.","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"The fourth, fifth and sixth columns contains the number of assets in the block, the mean number of ticks in the block and the mean time per tick. These do not do anything in the subsequent blockwise_estimation function but can be used to alter the dataframe. Now in the current case we may decide to estimate the block containing all assets using the spectral_covariance method.","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"one_asset_rows = findall(blocking_frame[:,:number_of_assets] .== 4)\nblocking_frame[one_asset_rows, :f] = spectral_covariance","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"We can now estimate the blockwise estimated CovarianceMatrix as:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"block_estimate = blockwise_estimation(ts_data, blocking_frame)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"After a blockwise estimation the result may often not be PSD. So we could regularise at this point:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"reg_block_estimate = nearest_correlation_matrix(block_estimate , ts_data)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"Finally we might seek to use one of our estimated CovarianceMatrixs to calculate an actual covariance matrix over some interval. This can be done with the code:","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"covariance_interval = 1000\ncovar = covariance(combined_estimate, covariance_interval)","category":"page"},{"location":"2_WritingCode/","page":"Using HighFrequencyCovariance","title":"Using HighFrequencyCovariance","text":"Note that the time units of the covariance_interval here should be the same units as the CovarianceMatrix struct's volatility which are the same units as the time dimension in the SortedDataFrame that is used to estimate the CovarianceMatrix.","category":"page"},{"location":"9_references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"9_references/","page":"References","title":"References","text":"Ait-Sahalia Y, Fan J, Xiu D (2010). “High-Frequency Covariance Estimates With Noisy and Asynchronous Financial Data.” Journal of the American Statistical Association, 105(492), 1504–1517. doi:10.1198/jasa.2010.tm10163.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Barndorff-Nielsen OE, Hansen PR, Lunde A, Shephard N (2011). “Multivariate realised kernels: consistent positive semi-definite estimators of the covariation of equity prices with noise and non-synchronous trading.” Journal of Econometrics, 162(2), 149–169. doi: 10.1016/j.jeconom.2010.07.009.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Bibinger M, Hautsch N, Malec P, Reiss M (2014). “Estimating the quadratic covariation matrix from noisy observations: Local method of moments and efficiency.” The Annals of Statistics, 42(4), 1312–1346. doi:10.1214/14-AOS1224.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Christensen K, Podolskij M, Vetter M (2013). “On covariation estimation for multivariate continuous Itô semimartingales with noise in non-synchronous observation schemes.” Jour- nal of Multivariate Analysis, 120, 59–84. doi:10.1016/j.jmva.2013.05.002.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Eps T (1979). “Comovements in stock prices in the very short run.” Journal of the American Statistical Association, 74, 291–296. doi:10.1080/01621459.1979.10482508.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Hautsch N, Kyj LM, Oomen RCA (2012). “A blocking and regularization approach to high- dimensional realized covariance estimation.” Journal of Applied Econometrics, 27(4), 625–","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"doi:10.1002/jae.1218.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Higham NJ (2002). “Computing the nearest correlation matrix - a problem from finance.” IMA Journal of Numerical Analysis, 22, 329–343. doi:10.1002/nla.258.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Laloux L, Cizeau P, Bouchaud JP, Potters M (1999). “Noise Dressing of Financial Correlation Matrices.” Physical Review Letters, 83(7), 1467–1470. ISSN 1079-7114. doi:10.1103/ physrevlett.83.1467.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Ledoit O, Wolf M (2001). “A well-conditioned estimator for large-dimensional covariance ma- trices.” Journal of Multivariate Analysis, 88(2), 365–411. doi:10.1016/S0047-259X(03) 00096-4.","category":"page"},{"location":"9_references/","page":"References","title":"References","text":"Zhang L, Mykland PA, Aït-Sahalia Y (2005). “A Tale of Two Time Scales: Determining Integrated Volatility with Noisy High-Frequency Data.” Journal of the American Statistical Association, 100(472), 1394–1411. ISSN 01621459. doi:10.1198/016214505000000169.","category":"page"},{"location":"#HighFrequencyCovariance","page":"HighFrequencyCovariance","title":"HighFrequencyCovariance","text":"","category":"section"},{"location":"","page":"HighFrequencyCovariance","title":"HighFrequencyCovariance","text":"This package estimates covariance matrix using high frequency data.","category":"page"},{"location":"","page":"HighFrequencyCovariance","title":"HighFrequencyCovariance","text":"This task is more complicated than normal covariance estimation due to several features of high frequency financial data:","category":"page"},{"location":"","page":"HighFrequencyCovariance","title":"HighFrequencyCovariance","text":"Assets are traded nonsyncronously. For instance we get an updated GOOG price at 14:28:00 and then get an updated price for MSFT only at 14:28:04. Ignoring the asynchronisity with which we get price updates can downwards bias correlations (Eps 1979).\nAssets may be traded over different intervals. For instance in calculating the correlation between JP Morgan and Credit Suisse we might have some days where Credit Suisse does  trade in Zurich but due to thanksgiving JP Morgan is not being traded in New York. So we might need to assemble a covariance matrix where the pairwise covariances/correlations come from slightly different intervals.\nPrice updates typically contain some \"microstructure noise\" which reflects frictions in the market rather than the longterm correlations between assets.\nThe microstruture noise can often not be iid but can exhibit serial correlation.\nOften more advanced techniques that adjust for the above issues are not guaranteed to return a PSD covariance matrix. So we need to regularise.","category":"page"},{"location":"","page":"HighFrequencyCovariance","title":"HighFrequencyCovariance","text":"HighFrequencyCovariance contains 2 volatility estimators, 5 covariance estimators, 4 regularisation techniques and a number of convenience functions that are intended to overcome these issues and produce reliable correlation, volatility and covariance estimates given high frequency financial data.","category":"page"},{"location":"","page":"HighFrequencyCovariance","title":"HighFrequencyCovariance","text":"A paper briefly outlining each technique is accessible. This paper also contains references to the original econometrics papers for users that seek a more detailed understanding.  The paper also contains a Monte Carlo analysis of the accuracy and time complexity of each algorithm. This documentation will not replicate all of that content and will instead focus on the practical details on how to write code to use this package.","category":"page"}]
}
